{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alina-89/Academic_RAG/blob/main/Academic_RAG_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzximbfgU10i"
      },
      "outputs": [],
      "source": [
        "!pip install gradio pymupdf sentence-transformers\n",
        "!pip install chromadb\n",
        "!pip install torch transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF9qh-otiPgT"
      },
      "outputs": [],
      "source": [
        "!pip install nbstripout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7nC9nAHU4eR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import fitz\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByymcd8zDiE"
      },
      "source": [
        "#Extract the text from the PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-vThP6QeiWV"
      },
      "outputs": [],
      "source": [
        "#Function that extracts text from the pdf\n",
        "def extract_text(pdf_file):\n",
        "  doc=fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
        "  text=\"\"\n",
        "  for page in doc:\n",
        "    text+=page.get_text()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LML3Og-WzJPO"
      },
      "source": [
        "#Split the text into sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sL1B76jx_Fk"
      },
      "outputs": [],
      "source": [
        "#Split text into sentences\n",
        "def naive_sent_tokenize(text):\n",
        "    # Splits on ., ?, or ! followed by a space or end of string\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWHIo4zBnffj"
      },
      "source": [
        "# Combine sentences into chunks of max. 100 tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDv9dHhg0ZuD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Chunk the sentences\n",
        "\n",
        "def chunk_text_by_sentences(text, max_tokens=100):\n",
        "    sentences = naive_sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_length = 0\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence.split())\n",
        "        if current_length + sentence_length > max_tokens:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "            current_length = sentence_length\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence\n",
        "            current_length += sentence_length\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLsPkct524H1"
      },
      "source": [
        "#Import Chroma and create a collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-msI1_u20Me"
      },
      "outputs": [],
      "source": [
        "#Import Chroma and create a collection\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "client = chromadb.Client(Settings(\n",
        "    persist_directory=\"chroma_db\",  # where to store your DB files\n",
        "    anonymized_telemetry=False\n",
        "))\n",
        "\n",
        "# Create or get your collection\n",
        "collection = client.get_or_create_collection(name=\"pdf_chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitNSmX_zX5k"
      },
      "source": [
        "#Load the embedding model and embed the chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToOgBT3tzP7U"
      },
      "outputs": [],
      "source": [
        "#Load the embedding model\n",
        "\n",
        "embedding_model=SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Embed the text chunks\n",
        "def embed_text(text, file_name=\"document.pdf\"):\n",
        "    chunks = chunk_text_by_sentences(text)\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
        "        collection.add(\n",
        "            documents=[chunk],\n",
        "            embeddings=[embedding],\n",
        "            ids=[f\"{file_name}_{i}\"],\n",
        "            metadatas=[{\"chunk_index\": i, \"source\": file_name}]\n",
        "        )\n",
        "\n",
        "    return f\"Stored {len(chunks)} chunks in Chroma for {file_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrL_jQGc_-bW"
      },
      "source": [
        "#Load the LLM locally\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiDlrR7o6c-o"
      },
      "outputs": [],
      "source": [
        "# Load Phi-2 model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bKj4PAUMXhA"
      },
      "source": [
        "#User enters a query, the query is embedded, the prompt (style+user query+retrieved chunks) is sent to the model, and the model will generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di-W_F4P5t2-"
      },
      "outputs": [],
      "source": [
        "#Lets user enter a query, embedds the query and retrieves relevant chunks then the model generates based on query and chunks and also on style\n",
        "def chat_with_pdf(user_query, style, top_k=5):\n",
        "    if not user_query.strip():\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = embedding_model.encode([user_query])[0]\n",
        "\n",
        "    # Search in ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    if not results[\"documents\"]:\n",
        "        return \"No relevant chunks found.\"\n",
        "\n",
        "    # Prepare prompt\n",
        "    chunks = results[\"documents\"][0]\n",
        "    prompt_style = {\n",
        "        \"Academic\": \"Answer the question in an academic tone.\",\n",
        "        \"Friendly\": \"Answer the question like you are someone's best friend.\",\n",
        "        \"Explain like I am 5\": \"Answer the question in a simple way, like you're explaining to a 5 year old child.\"\n",
        "    }\n",
        "\n",
        "    prompt = f\"{prompt_style.get(style, '')}\\n\\nContext:\\n\" + \"\\n\\n\".join(chunks) + f\"\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer.split(\"Answer:\")[-1].strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXxCM7GbVgTp"
      },
      "outputs": [],
      "source": [
        "#Interface with gradio\n",
        "with gr.Blocks(css=\"\"\"\n",
        "/* 🌙 Dark Mode Background */\n",
        ".gradio-container {\n",
        "    background-color: #18181B;\n",
        "    color: #FFFFFF;\n",
        "    font-family: 'Poppins', sans-serif;\n",
        "    padding: 20px;\n",
        "    display: flex;\n",
        "    justify-content: center;\n",
        "}\n",
        "\n",
        "/* Stack content vertically and center */\n",
        "#main-column {\n",
        "    display: flex;\n",
        "    flex-direction: column;\n",
        "    align-items: center;\n",
        "    max-width: 800px;\n",
        "    width: 100%;\n",
        "    margin: auto;\n",
        "}\n",
        "\n",
        "/* 🖼️ Image Styling */\n",
        ".gr-image {\n",
        "    border-radius: 12px;\n",
        "    box-shadow: 4px 4px 10px rgba(255, 255, 255, 0.2);\n",
        "}\n",
        "\n",
        "/* ✏️ Textbox Enhancements */\n",
        ".gr-textbox {\n",
        "    width: 90%;\n",
        "    font-size: 18px;\n",
        "    padding: 10px;\n",
        "    border: 2px solid #4A4A4D;\n",
        "}\n",
        "\n",
        "/* 🎨 Button Customization */\n",
        ".gr-button {\n",
        "    background-color: #5A67D8;\n",
        "    color: pink;\n",
        "    font-size: 16px;\n",
        "    padding: 12px 18px;\n",
        "    border-radius: 8px;\n",
        "    transition: 0.2s ease-in-out;\n",
        "}\n",
        "\n",
        "/* ✨ Refine Labels */\n",
        "label {\n",
        "    font-weight: bold;\n",
        "    color: #D1D5DB;\n",
        "}\n",
        "\"\"\") as demo:\n",
        "    with gr.Column(elem_id=\"main-column\"):\n",
        "        gr.Markdown(\"<h2 style='color: #EAB308;'>📄 Upload your PDF</h2><p style='color: #9CA3AF;'>Then ask questions about it</p>\")\n",
        "\n",
        "        file_input = gr.File(label=\"Upload a file\", type=\"binary\")\n",
        "        style_input = gr.Radio([\"Academic\", \"Friendly\", \"Explain like I am 5\"], label=\"Choose a style\")\n",
        "        #upload_btn = gr.Button(\"Upload PDF\")\n",
        "\n",
        "        gr.Markdown(\"<h2 style='color: #34D399;'>💬 Ask a question</h2>\")\n",
        "        query_input = gr.Textbox(label=\"Your question\")\n",
        "        query_btn = gr.Button(\"Ask\")\n",
        "        query_output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "        query_btn.click(fn=chat_with_pdf,\n",
        "                        inputs=[query_input, style_input],\n",
        "                        outputs=[query_output])\n",
        "\n",
        "    demo.launch(debug=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}